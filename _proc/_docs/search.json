[
  {
    "objectID": "vae.html",
    "href": "vae.html",
    "title": "Variational AutoEncoder Models",
    "section": "",
    "text": "FashionMnistDataset\n\n FashionMnistDataset (csv_file='~/Data/fashion-mnist/fashion-\n                      mnist_train.csv')\n\nFashion MNIST Dataset\n\nmap_labels = {\n  \"0\": \"T-shirt/top\", \"1\": \"Trouser\", \"2\": \"Pullover\", \"3\": \"Dress\",\n  \"4\": \"Coat\", \"5\": \"Sandal\", \"6\": \"Shirt\", \"7\": \"Sneaker\", \"8\": \"Bag\",\n  \"9\": \"Ankle boot\"\n}\n\nds = FashionMnistDataset()\ndl = torch.utils.data.DataLoader(ds, batch_size=128, shuffle=True)\n\nX, y = next(iter(dl))\nprint(X.type(), y.type())\n\nplt.figure(figsize = (1, 1))\nbatch_n = 0\nplt.imshow(X[batch_n].numpy().reshape(28,28),cmap='gray')\nplt.title(map_labels[str(int(y[batch_n]))])\nplt.show()\nprint(f\"shape of image batch: {X.shape}, Shape of labels: {y.shape}\")\n\ntorch.DoubleTensor torch.IntTensor\n\n\n\n\n\nshape of image batch: torch.Size([128, 784]), Shape of labels: torch.Size([128])\n\n\n\n\n\nEncoder\n\n Encoder (latent_dim=2)\n\nEncoder\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlatent_dim\nint\n2\nLatent dimension\n\n\n\n\nprint(X.type())\ne = Encoder()\nee = e(X.type(torch.FloatTensor))\nprint(ee.shape, ee.type())\n\ntorch.DoubleTensor\ntorch.Size([128, 2]) torch.FloatTensor\n\n\n\n\n\nDecoder\n\n Decoder (latent_dim:int=2)\n\nDecoder\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlatent_dim\nint\n2\nLatent dimension\n\n\n\n\n\n\nAutoEncoder\n\n AutoEncoder (latent_dim:int=2)\n\nautoencoder\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlatent_dim\nint\n2\nLatent dimension"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "slg_generative",
    "section": "",
    "text": "Looking at foundation of generative AI, starting with Variational AutoEncoders. More to come later!"
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "slg_generative",
    "section": "Install",
    "text": "Install\npip install slg_generative"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "slg_generative",
    "section": "How to use",
    "text": "How to use\nSome example usage of the library\n\nImports\n\nfrom slg_generative.data.datasets import FashionMnistDataset\nfrom slg_generative.models.vae import AutoEncoder\nfrom slg_generative.training import Trainer\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.optim import Adam\nimport torch.nn as nn\n\n\n\nData\n\nds = FashionMnistDataset(csv_file=\"~/Data/fashion-mnist/fashion-mnist_train.csv\")\ndl = torch.utils.data.DataLoader(ds,batch_size=128, shuffle=True, num_workers=0)\n\n\n\nModel\n\ndevice = 'mps' if torch.backends.mps.is_available() else 'cpu' # or 'cuda' for nvidia gpus\nautoencoder = AutoEncoder().to(device)\n\n\n\nTraining Setup\n\nopt = Adam(autoencoder.parameters(), lr=1e-3)\nloss_func = nn.MSELoss()\nn_epochs = 5\n\n\n\nTraining Loop\n\ntrainer = Trainer(autoencoder, dl, loss_func, opt, n_epochs, device)\ntrainer.fit()"
  },
  {
    "objectID": "models.vae.html",
    "href": "models.vae.html",
    "title": "VAE Models",
    "section": "",
    "text": "source\n\nAEEncoder\n\n AEEncoder (input_dim:int=784, hidden_dim:int=512, latent_dim:int=2)\n\nSimple AE Encoder\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninput_dim\nint\n784\nInput dimension\n\n\nhidden_dim\nint\n512\nHidden dimension\n\n\nlatent_dim\nint\n2\nLatent dimension\n\n\n\n\nds = FashionMnistDataset(\"~/Data/fashion-mnist/fashion-mnist_train.csv\")\ndl = torch.utils.data.DataLoader(ds, batch_size=128, shuffle=True)\nX, y = next(iter(dl))\ne = AEEncoder(input_dim=784, hidden_dim=512, latent_dim=2)\nee = e(X)\nprint(f\"latent output shape {ee.shape}, {ee.type()}\")\n\nlatent output shape torch.Size([128, 2]), torch.FloatTensor\n\n\n\nsource\n\n\nAEDecoder\n\n AEDecoder (latent_dim=2, hidden_dim=512, output_dim=784)\n\nDecoder\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlatent_dim\nint\n2\nLatent dimension\n\n\nhidden_dim\nint\n512\nHidden dimension\n\n\noutput_dim\nint\n784\nOutput dimension\n\n\n\n\nd = AEDecoder(latent_dim=2, hidden_dim=512, output_dim=784)\ndd = d(ee)\nprint(dd.shape)\n\ntorch.Size([128, 784])\n\n\n\nsource\n\n\nAutoEncoder\n\n AutoEncoder (input_dim=784, hidden_dim=512, latent_dim=2)\n\nautoencoder\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninput_dim\nint\n784\nOutput dimension\n\n\nhidden_dim\nint\n512\nHidden dimension\n\n\nlatent_dim\nint\n2\nLatent dimension\n\n\n\n\nae = AutoEncoder(input_dim=784, hidden_dim=512, latent_dim=2)\nout = ae(X)\nprint(out.shape)\n\ntorch.Size([128, 784])"
  },
  {
    "objectID": "data.datasets.html",
    "href": "data.datasets.html",
    "title": "Datasets",
    "section": "",
    "text": "source\n\nImageDataset\n\n ImageDataset ()\n\nBase class for image datasets providing visualization of (image, label) samples\n\nsource\n\n\nMnistDataset\n\n MnistDataset (data_root:str='./data')\n\nMNIST digit dataset\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata_root\nstr\n./data\nPath to data root folder\n\n\n\n\nds = MnistDataset('~/Data')\nprint(f\"Number of samples in the dataset: {len(ds)}\")\nX,y = ds[0]\nprint(f\"Shape of image: {X.shape}, corresponding digit: {int(y)}\")\nprint(f\"types: {X.type()}, {y.type()}\")\n\nds.show(0)\n\nNumber of samples in the dataset: 60000\nShape of image: torch.Size([784]), corresponding digit: 5\ntypes: torch.FloatTensor, torch.LongTensor\n\n\n\n\n\n\nsource\n\n\nFashionMnistDataset\n\n FashionMnistDataset (csv_file:str)\n\nFashion MNIST Dataset\n\n\n\n\nType\nDetails\n\n\n\n\ncsv_file\nstr\nPath to csv data file\n\n\n\n\nmap_labels = {\n  \"0\": \"T-shirt/top\", \"1\": \"Trouser\", \"2\": \"Pullover\", \"3\": \"Dress\",\n  \"4\": \"Coat\", \"5\": \"Sandal\", \"6\": \"Shirt\", \"7\": \"Sneaker\", \"8\": \"Bag\",\n  \"9\": \"Ankle boot\"\n}\n\nds = FashionMnistDataset(csv_file=\"~/Data/fashion-mnist/fashion-mnist_train.csv\")\nprint(f\"Number of samples in the dataset: {len(ds)}\")\nX,y = ds[0]\n\nprint(f\"Shape of image: {X.shape}, corresponding label: {map_labels[str(int(y))]}\")\nds.show(0)\n\ndl = torch.utils.data.DataLoader(ds, batch_size=128, shuffle=True)\n\nX, y = next(iter(dl))\nprint(f\"types of batch content: {X.type()}, {y.type()}\")\n\nNumber of samples in the dataset: 60000\nShape of image: torch.Size([784]), corresponding label: Pullover\n\n\n\n\n\ntypes of batch content: torch.FloatTensor, torch.LongTensor"
  },
  {
    "objectID": "training.html",
    "href": "training.html",
    "title": "Training",
    "section": "",
    "text": "source\n\nTrainer\n\n Trainer (model:slg_generative.models.vae.AutoEncoder,\n          dataloader:torch.utils.data.dataloader.DataLoader,\n          loss_func:torch.nn.modules.loss._Loss,\n          optimizer:torch.optim.optimizer.Optimizer, n_epochs:int,\n          device:str)\n\nTrainer for VAE models\n\n\n\n\nType\nDetails\n\n\n\n\nmodel\nAutoEncoder\nModel\n\n\ndataloader\nDataLoader\nDataloader\n\n\nloss_func\n_Loss\nLoss function\n\n\noptimizer\nOptimizer\nOptimizer\n\n\nn_epochs\nint\nNumber of training epochs\n\n\ndevice\nstr\nDevice\n\n\n\n\n# device\ndevice = 'mps' if torch.backends.mps.is_available() else 'cpu' # or 'cuda' for nvidia gpus\n# data\nds = FashionMnistDataset(csv_file=\"~/Data/fashion-mnist/fashion-mnist_train.csv\")\ndl = torch.utils.data.DataLoader(ds,batch_size=128,shuffle=True)\n# model\nautoencoder = AutoEncoder().to(device)\n# training params\nn_epochs = 5\n# optim\nopt = Adam(autoencoder.parameters(), lr=1e-3)\n# # mean square error loss \nloss_func = nn.MSELoss()\n\n\ntrainer = Trainer(autoencoder, dl, loss_func, opt, n_epochs, device)\ntrainer.fit()\n\n  0%|          | 0/5 [00:00<?, ?it/s]\n\n\n Train Epoch: 1/5 [59776/60000 (100%)]  Loss: 0.634618\n\n\n 20%|██        | 1/5 [00:04<00:19,  4.86s/it]\n\n\n Train Epoch: 2/5 [56960/60000 (95%)]   Loss: 0.6342823\n\n\n 40%|████      | 2/5 [00:07<00:10,  3.64s/it]\n\n\n Train Epoch: 3/5 [57984/60000 (97%)]   Loss: 0.6208118\n\n\n 60%|██████    | 3/5 [00:10<00:06,  3.26s/it]\n\n\n Train Epoch: 4/5 [59008/60000 (98%)]   Loss: 0.6307802\n\n\n 80%|████████  | 4/5 [00:13<00:03,  3.08s/it]\n\n\n Train Epoch: 5/5 [57216/60000 (95%)]   Loss: 0.6383358\n\n\n100%|██████████| 5/5 [00:16<00:00,  3.20s/it]\n\n\n Train Epoch: 5/5 [44928/60000 (100%)]  Loss: 0.604993"
  }
]