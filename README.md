Generative
================

<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

Looking at foundation of generative AI, starting with Variational
AutoEncoders. More to come later!

## Install

``` sh
pip install slg_generative
```

## How to use

Some example usage of the library

### Data

``` python
from slg_generative.data.datasets import FashionMnistDataset
from torch.utils.data import DataLoader

ds = FashionMnistDataset(csv_file="~/Data/fashion-mnist/fashion-mnist_train.csv")
dl = DataLoader(ds,batch_size=128, shuffle=True, num_workers=0)
val_ds = FashionMnistDataset(csv_file="~/Data/fashion-mnist/fashion-mnist_test.csv")
val_dl = DataLoader(val_ds,batch_size=128, shuffle=True, num_workers=0)
```

### Model

``` python
import torch
from slg_generative.models.vae import AutoEncoder

device = 'mps' if torch.backends.mps.is_available() else 'cpu' # or 'cuda' for nvidia gpus
autoencoder = AutoEncoder().to(device)
```

### Training Setup

``` python
from torch.optim import Adam
import torch.nn as nn

opt = Adam(autoencoder.parameters(), lr=1e-3)
loss_func = nn.MSELoss()
n_epochs = 5
```

### Training Loop

``` python
from slg_generative.training import Trainer

trainer = Trainer(autoencoder, dl, val_dl, loss_func, opt, n_epochs, device)
trainer.fit()
```

      0%|          | 0/5 [00:00<?, ?it/s]

     Train Epoch: 1/5 [51072/60000 (85%)]   Loss: 0.622993

     20%|██        | 1/5 [00:02<00:11,  2.91s/it]

     Validion Loss: 0.6280178548414496
     Train Epoch: 2/5 [51072/60000 (85%)]   Loss: 0.619879

     40%|████      | 2/5 [00:05<00:08,  2.90s/it]

     Validion Loss: 0.6231143165238296
     Train Epoch: 3/5 [51072/60000 (85%)]   Loss: 0.620355

     60%|██████    | 3/5 [00:08<00:05,  2.99s/it]

     Validion Loss: 0.621142838574663
     Train Epoch: 4/5 [51072/60000 (85%)]   Loss: 0.603328

     80%|████████  | 4/5 [00:13<00:03,  3.55s/it]

     Validion Loss: 0.6198473678359503
     Train Epoch: 5/5 [51072/60000 (85%)]   Loss: 0.631183

    100%|██████████| 5/5 [00:16<00:00,  3.40s/it]

     Validion Loss: 0.6195422592042368
